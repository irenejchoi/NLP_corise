{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Irene NLP- Week 3 - Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOTJFRLT5i1Y"
      },
      "source": [
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "\n",
        "# Week 3: Embedding-Based Retrieval\n",
        "\n",
        "### What we are building\n",
        "The goal of Embedding-Based Retrieval is to retrieve top-k candidates given a query based on embedding similarity/distance. A common application for this is given a query/sentence/document, find top-k similar candidates wrt query. While this is usually solved using TF-IDF/Information Retrieval (IR) based approaches, it is becoming more and more common in the industry to use an embedding based approach: encode the query and document as an embedding and use approximate nearest neighbor search to find top-k candidates in real-time.\n",
        "\n",
        "We will build a system to find duplicate questions on Quora using a [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs). A very common problem for forums/QA websites is trying to determine whether a question has already been asked before a user posts it.\n",
        "\n",
        "We will continue to apply our learning philosophy of repetition as we build multiple models of increasing complexity in the following order:\n",
        "\n",
        "1. Retrieval based on WordVectors\n",
        "1. Using BERT\n",
        "1. Using Sentence BERT\n",
        "1. Using Cohere Sentence Embeddings\n",
        "\n",
        "###  Evaluation\n",
        "We will evaluate our models along the following metrics: \n",
        "\n",
        "1. Recall@k: the proportion of relevant items found in the top-k matches\n",
        "1. Mean Reciprocal Rank: the rank of the first relevant item with respect to the top-k.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. We have provide scaffolding for all the boiler plate Faiss code to get to our baseline model. This covers downloading and parsing the dataset, and training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
        "1. At this point in our model, we will aim to use BERT embeddings. **Does this improve accuracy?**\n",
        "1. In the third model, we will use Sentence BERT and then we'll see if they can boost up our model. **How do you think this model will perform?**\n",
        "1. **Extension**: We have suggested a bunch of extensions to the project so go crazy! Tweak any parts of the pipeline, and see if you can beat all the current modes.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "- Dependencies: Install and import python dependencies\n",
        "- Project\n",
        "  - Dataset: Download the Quora dataset\n",
        "  - Indexer: Function to manage and create a Faiss Index\n",
        "  - Model 1: Word Vectors\n",
        "  - Model 2: BERT\n",
        "  - Model 3: Sentence BERT\n",
        "  - Model 4: Cohere Sentence Embeddings\n",
        "- Extensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zLCEfd7VKI"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhbV2UD5UGd"
      },
      "source": [
        "%%capture\n",
        "# Install all the required dependencies for the project\n",
        "!pip install pytorch-lightning==1.5.10\n",
        "!pip install spacy==2.2.4\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt install libopenblas-base libomp-dev\n",
        "!pip install faiss==1.5.3\n",
        "!pip install transformers==4.17.0\n",
        "!pip install sentence-transformers==2.2.0\n",
        "!pip install cohere"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTWZJAqiBxEv"
      },
      "source": [
        "Import all the necessary libraries we need throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esA3TFU2-9dI"
      },
      "source": [
        "# Import all the relevant libraries\n",
        "import csv\n",
        "import en_core_web_md\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import random\n",
        "import spacy\n",
        "import torch\n",
        "import cohere\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertTokenizer, BertModel, BertTokenizerFast, DistilBertTokenizer, DistilBertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWE7zx6Wnria"
      },
      "source": [
        "Now let's load the Spacy data, which comes with pre-trainined embeddings. This process is expensive so only do it once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhsykZdEK2m6"
      },
      "source": [
        "# Really expensive operation to load the entire space word-vector index in memory\n",
        "# We'll only run it once \n",
        "loaded_spacy_model = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiXEUsahCJeA"
      },
      "source": [
        "# Embedding Based Retrieval\n",
        "\n",
        "âœ¨ Let's Begin âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFcN6rKkCQiu"
      },
      "source": [
        "### Data Loading and Processing (Common to ALL Solutions)\n",
        "\n",
        "#### Dataset\n",
        "\n",
        "Download the duplicate questions [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTXv0v34AYOU"
      },
      "source": [
        "%%capture\n",
        "!wget 'http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv'\n",
        "!mkdir qqp\n",
        "!mv quora_duplicate_questions.tsv qqp/\n",
        "!ls qqp/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjVbbS-CucF0"
      },
      "source": [
        "Perfect. Now we see all of our files. Let's poke at one of them before we start parsing our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-MUggjUui6y"
      },
      "source": [
        "DATA_FILE = \"qqp/quora_duplicate_questions.tsv\"\n",
        "\n",
        "# The file is a 6-column tab separated file. \n",
        "# The first column is the row_id, second and third questions are ids of \n",
        "# specific questions, followed by the text of questions.\n",
        "# The last column captures if the two questions are duplicates\n",
        "with open(DATA_FILE, 'r', newline='\\n') as file:\n",
        "  reader = csv.reader(file, delimiter = '\\t')\n",
        "  # Read first 10 lines\n",
        "  for i in range(10):\n",
        "    print(next(reader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5uNuyGSxRql"
      },
      "source": [
        "The dataset has more than 500k questions! We are going to parse the full dataset and create a sample of 10k questions to experiment with in our models since BERT training & inference can be really slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_mRCola0s8z"
      },
      "source": [
        "\"\"\"\n",
        "Util function to parse the file\n",
        "\"\"\"\n",
        "def parse_sample_dataset(file_path, sample_max_id):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    file_path: Path to the raw data file\n",
        "    sample_max_id: Max question id to be considered in the sampled dataset\n",
        "\n",
        "  Returns 4 objects:\n",
        "    1. QuestionMap: list of all question ids\n",
        "    2. DuplicatesMap: Map of questionID to it's duplicates\n",
        "    3. SampleDataset: list of questionIds in the sample\n",
        "    4. SampleEvalDataset: list of pair of duplicate questions in the sample\n",
        "  \"\"\"\n",
        "  question_map = {}\n",
        "  duplicates_map = defaultdict(set)\n",
        "  sample_dataset = set([])\n",
        "  sample_eval_dataset = []\n",
        "\n",
        "  with open(file_path, 'r', newline='\\n') as file:\n",
        "    reader = csv.reader(file, delimiter='\\t')\n",
        "    next(reader)  # Skip the header line\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) != 6: # Skip incomplete rows\n",
        "        continue\n",
        "\n",
        "      # Limit the sample size of the dataset at max_id\n",
        "      # Make sure all 4 objects start at index 0\n",
        "      qid1, qid2, label = int(row[1]) - 1, int(row[2]) - 1, int(row[5])\n",
        "      if qid1 < sample_max_id and qid2 < sample_max_id:\n",
        "        \n",
        "        if qid1 not in question_map:\n",
        "          question_map[qid1] = str(row[3])\n",
        "        if qid2 not in question_map:\n",
        "          question_map[qid2] = str(row[4])\n",
        "\n",
        "        if label == 1:\n",
        "          duplicates_map[qid1].add(qid2)\n",
        "          duplicates_map[qid2].add(qid1)\n",
        "\n",
        "          sample_eval_dataset.append((qid1, qid2))\n",
        "\n",
        "        sample_dataset.add(qid1)\n",
        "        sample_dataset.add(qid2)\n",
        "\n",
        "  # sample dataset duplicates removed via set(), so turn back into list\n",
        "  return question_map, duplicates_map, list(sample_dataset), sample_eval_dataset\n",
        "\n",
        "question_map, duplicates_map, sample_dataset, sample_eval_dataset, = parse_sample_dataset(DATA_FILE, 10000)\n",
        "\n",
        "# Complete file: 537k unique questions, 400k duplicate.\n",
        "# To keep training time manageable limited to 10.000 (sample_max_id)\n",
        "print(\"Number of unique questions:\", len(question_map)) # 10.000\n",
        "print(\"Number of question with duplicates:\", len(duplicates_map)) # ~3.8k\n",
        "print(\"Number of questions in sample:\", len(sample_dataset)) # 10.000\n",
        "print(\"Number of duplicate pairs in sample:\", len(sample_eval_dataset)) # ~3.6k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC9D41185-Oa"
      },
      "source": [
        "# Retrieval using Faiss -- TO BE COMPLETED\n",
        "\n",
        "You are now going to create an Indexer class that implements multiple functions for indexing, searching, and evaluating our retrieval model. Faiss documentation can be found in the wiki here: https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
        "\n",
        "Some helpful Faiss guides are:\n",
        "- https://www.pinecone.io/learn/faiss-tutorial/\n",
        "- https://www.pinecone.io/learn/vector-indexes/\n",
        "\n",
        "You need to implement the following functions:\n",
        "\n",
        "1. **search**: Implement a function that takes a question and top_k variable and returns either the matched strings or the ids to the user as a \n",
        "    1. Call the search API on the faiss_index to look up similar sentences using `faiss_index.search`\n",
        "    2. Parse the output to either return [sentence_id, score] tuples or [sentence, score] tuples based on the input parameter\n",
        "    3. Sort the output by the score in descending order\n",
        "\n",
        "1. **evaluate**: Sample num_docs pairs from the evaluation dataset and then check if the qid2 is present in the top-k results\n",
        "    1. For each eval sample, find the top_k matches for the qid1\n",
        "    2. See if the qid2 is in one of the matches\n",
        "    3. If yes, append (1) to the recall array otherwise append (0)\n",
        "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji7yyBk5Ou6k"
      },
      "source": [
        "class FaissIndexer:\n",
        "  def __init__(self, dataset,\n",
        "               question_map, \n",
        "               eval_dataset, \n",
        "               batch_size, \n",
        "               sentence_vector_dim, \n",
        "               vectorizer):\n",
        "    self.question_map = question_map\n",
        "    self.dataset = dataset\n",
        "    self.eval_dataset = eval_dataset\n",
        "    self.batch_size = batch_size\n",
        "    self.vectorizer = vectorizer\n",
        "    # FlatIP uses L2 distance\n",
        "    self.faiss_index = faiss.IndexFlatIP(sentence_vector_dim)\n",
        "\n",
        "\n",
        "  def split_list(self, lst = list, sublist_size = int):\n",
        "    sublists = []\n",
        "    # Split lst into even chunks/sublists/batches\n",
        "    for i in range(0, len(lst), sublist_size): \n",
        "        sublists.append(lst[i:i + sublist_size])\n",
        "    return sublists\n",
        "\n",
        "\n",
        "  def index(self):\n",
        "    sentence_vectors = []\n",
        "\n",
        "    print(\"Start indexing!\")\n",
        "    for sentence_ids in tqdm(self.split_list(self.dataset, self.batch_size)):\n",
        "      # Retrieve sentences based on qid\n",
        "      sentences = [question_map[qid] for qid in sentence_ids]\n",
        "      # Get embeddings of the sentences (Spacy, ..., Cohere)\n",
        "      sentence_vectors_batch = self.vectorizer.vectorize(sentences)\n",
        "      # Add batch to temporary list\n",
        "      sentence_vectors.append(sentence_vectors_batch)\n",
        "\n",
        "    # Add all batches from temporary list to index\n",
        "    self.faiss_index.add(np.array(np.concatenate(sentence_vectors, axis=0)))\n",
        "    print(\"\\nDone indexing!\")\n",
        "\n",
        "\n",
        "  def search(self, question: str, top_k: int, return_ids=False):\n",
        "    \"\"\"Given any sentence (typed by the user)\n",
        "    We return a list of top_k(sentence, sim_score) or top_k(sentence_ids, sim_score)\n",
        "    \n",
        "    NOTE: The output type is controlled by the return_ids flag\n",
        "\n",
        "    1. Call the search API on the faiss_index to look up similar sentences \n",
        "       using `faiss_index.search`\n",
        "    2. Parse the output to either return [sentence_id, score] tuples or \n",
        "       [sentence, score] tuples based on return_ids being true/false\n",
        "    3. Sort the output by the score in descending order\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: We converted the question to a list here to match the signature \n",
        "    # of the vectorize function\n",
        "    question_vectors = self.vectorizer.vectorize([question])\n",
        "\n",
        "    ### TO BE IMPLEMENTED ###\n",
        "    scores, indices = self.faiss_index.search(question_vectors,top_k)\n",
        "\n",
        "    # Output is a List[(qid, score), (qid, score), (qid, score)] or \n",
        "    # List[(q, score), (q, score), (q, score)] based on return_ids\n",
        "    # Output is sorted in descending order of score\n",
        "\n",
        "    if return_ids:\n",
        "      output = [(self.dataset[i],s) for s,i in zip(scores[0],indices[0])]\n",
        "    else:\n",
        "      output = [(question_map[i],s) for s,i in zip(scores[0],indices[0])]\n",
        "    return output\n",
        "\n",
        "\n",
        "  def evaluate(self, top_k: int, eval_sample_size: int):\n",
        "    \"\"\"Sample num_docs pairs from the evaluation dataset and then check \n",
        "    if the qid2 is present in the top-k results\n",
        "\n",
        "    1. For each eval sample, find the top_k matches for the qid1\n",
        "    2. See if the qid2 is in one of the matches\n",
        "    3. If yes, append (1) to the recall array otherwise append (0)\n",
        "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches\n",
        "      - Note: MRR is equivalent to mean([1/r or 0 for each sample])\n",
        "    \"\"\"\n",
        "    # Sample from evaluation dataset as proxy for performance metrics\n",
        "    eval_sample = random.sample(self.eval_dataset, eval_sample_size)\n",
        "\n",
        "    # Retrieval metrics which only care about if searched for\n",
        "    # item is present among the results.\n",
        "    recall_at_k = [] # Relevant items vs total of relevant items\n",
        "    mean_reciprocal_rank = [] # Rank of the first relevant item\n",
        "\n",
        "    ### TO BE IMPLEMENTED ### \n",
        "    for qid1,qid2 in eval_sample:\n",
        "      question = self.question_map[qid1]\n",
        "      searched = self.search(question,top_k,return_ids=True)\n",
        "      # increment matches but idk how?\n",
        "      matches = []\n",
        "      if qid2 in matches:\n",
        "          recall_at_k.append(1)\n",
        "          matchid = matches.index(qid2)\n",
        "          mean_reciprocal_rank.append(1/(matchid+1))\n",
        "      else:\n",
        "          recall_at_k.append(0)\n",
        "          mean_reciprocal_rank.append(0)\n",
        "\n",
        "    print(\"\\nRecall@{}:\\t\\t{:0.2f}%\".format(top_k, np.mean(np.array(recall_at_k) * 100.0)))\n",
        "    print(\"Mean Reciprocal Rank:\\t{:0.2f}\".format(np.mean(np.array(mean_reciprocal_rank))))\n",
        "\n",
        "\n",
        "  # Helper function to train, search and evaluate similar output from all the models created.\n",
        "  def train_and_evaluate(self, \n",
        "                         question_example: str, \n",
        "                         top_k: int = 10, \n",
        "                         eval_sample_size: int = 1000\n",
        "                         ):\n",
        "    print(\"---- Indexing ----\")\n",
        "    self.index()\n",
        "    print(\"\\n---- Search ----\")\n",
        "    results = self.search(question_example, top_k, return_ids=False)\n",
        "    print(\"Questions similar to:\", question_example)\n",
        "    for i, (q, s) in enumerate(results):\n",
        "      print(f\"{i} Question: {q} with score {s}\")\n",
        "    print(\"\\n---- Evaluation ----\")\n",
        "    self.evaluate(top_k, eval_sample_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuMWzJZjpKdW"
      },
      "source": [
        "## Dummy Model Test\n",
        "\n",
        "Really small sample of 4 sentences to make sure we can test our implementation of the FAISS search function correctly. We just project the 4 questions in a 2-d space where they are placed on the X-Axis if the word `invest` is present and on the Y-axis if `kohinoor` is present. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UmvNFIIw1eO"
      },
      "source": [
        "dummy_ids = sample_dataset[:4]\n",
        "print(\"Questions:\")\n",
        "for i in dummy_ids:\n",
        "  print(i, \":\", question_map[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8n4UBEZpPT1"
      },
      "source": [
        "class DummyVectorizer:\n",
        "  def __init__(self, sentence_vector_dim):\n",
        "    self.sentence_vector_dim = sentence_vector_dim\n",
        "\n",
        "  def vectorize(self, sentences):\n",
        "    \"\"\"Return sentence vectors for the batch of sentences. \n",
        "\n",
        "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
        "    2. Sentence vector is the mean of word vectors of each token\n",
        "    3. Stack the sentence vectors into a numpy array using np.stack\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "      if \"invest\" in sentence:\n",
        "        # If \"invest\" is present place it on the X-Axis\n",
        "        vectors.append(np.array([random.random(), 0], dtype=np.float32))\n",
        "      elif \"Kohinoor\" in sentence:\n",
        "        # If \"Kohinoor\" is present place it on the Y-Axis\n",
        "        vectors.append(np.array([0, random.random()], dtype=np.float32))\n",
        "    return np.stack(vectors)\n",
        "\n",
        "\n",
        "di = FaissIndexer(dummy_ids, \n",
        "                  question_map,\n",
        "                  sample_eval_dataset,\n",
        "                  batch_size=1024, \n",
        "                  sentence_vector_dim=2, \n",
        "                  vectorizer=DummyVectorizer(2)\n",
        "                  )\n",
        "\n",
        "di.index()\n",
        "\n",
        "results = di.search(\"invest\", 4)\n",
        "print(\"Questions similar to:\", \"invest\")\n",
        "for i, (q, s) in enumerate(results):\n",
        "  print(f\"{i} Question: {q} with score {s}\")\n",
        "\n",
        "results = di.search(\"Kohinoor\", 4)\n",
        "print(\"\\nQuestions similar to:\", \"Kohinoor\")\n",
        "for i, (q, s) in enumerate(results):\n",
        "  print(f\"{i} Question: {q} with score {s}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtZuKF-Y3kkB"
      },
      "source": [
        "# Models\n",
        "\n",
        "You may be wondering, \"When are we going to start building models?\" And, the answer is NOW! Finally the time has come to build our baseline model, and then we'll work towards improving it. \n",
        "\n",
        "\n",
        "**NOTE**: We will be using the sample dataset since BERT is really slow and processing the full dataset will take a lot of time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFesUhmewgGY"
      },
      "source": [
        "### Model 1: Averaging Word Vectors --- TO BE COMPLETED\n",
        "##### <font color='red'>Expected recall@10: ~20%, MRR: ~0.07</font>\n",
        "\n",
        "Complete the `vectorize` function using Spacy provided word embeddings. This is something we've done twice already :) \n",
        "\n",
        "Implementation:\n",
        "\n",
        "1. Tokenize each sentence and get wordVectors for each token in the sentence using Spacy \n",
        "2. Sentence vector is the mean of word vectors of each token\n",
        "3. Stack the sentence vectors into a numpy array using np.stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ahi3pH_Ce6c"
      },
      "source": [
        "class SpacyVectorizer:\n",
        "  def __init__(self, sentence_vector_dim):\n",
        "    self.sentence_vector_dim = sentence_vector_dim\n",
        "\n",
        "  def vectorize(self, sentences):\n",
        "    \"\"\"Return sentence vectors for the batch of sentences. \n",
        "\n",
        "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
        "    2. Sentence vector is the mean of word vectors of each token\n",
        "    3. Stack the sentence vectors into a numpy array using np.stack\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "\n",
        "      ### TO BE COMPLETED ###\n",
        "      spacy_doc = loaded_spacy_model.make_doc(sentence)\n",
        "      word_vector = [token.vector for token in spacy_doc]\n",
        "      sentence_vector = np.mean(np.array(word_vector),axis=0)\n",
        "\n",
        "      vectors.append(sentence_vector)\n",
        "    return np.stack(vectors)\n",
        "\n",
        "\n",
        "spacyIndex = FaissIndexer(sample_dataset,\n",
        "                  question_map,\n",
        "                  sample_eval_dataset,\n",
        "                  batch_size=1024, \n",
        "                  sentence_vector_dim=300, \n",
        "                  vectorizer=SpacyVectorizer(300))\n",
        "\n",
        "spacyIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHoRuwCOwhiH"
      },
      "source": [
        "### Model 2: BERT Embeddings --- TO BE COMPLETED\n",
        "##### <font color='red'>Expected recall@10: ~48%, MRR: ~0.19</font>\n",
        "\n",
        "Compute the sentence embeddings using the BERT model and complete the `vectorize` function. Feel free to reference any documentation from https://huggingface.co/. \n",
        "\n",
        "\n",
        "Implementation:\n",
        "\n",
        "1. Tokenize batch of sentences using `self.tokenizer`\n",
        "2. Pipe the inputs through the BERT model to create the output logits\n",
        "3. Normalize the batch output\n",
        "\n",
        "**NOTE: This model is really slow and will take about 20 mins to run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAWOt3cTC9Ig"
      },
      "source": [
        "class BertVectorizer:\n",
        "  def __init__(self):\n",
        "    self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "  def vectorize(self, sentences):\n",
        "    \"\"\"Return sentence vectors for the batch of sentences. \n",
        "\n",
        "    1. Tokenize batch of sentences using `self.tokenizer`\n",
        "    2. Pipe the inputs through the BERT model to create the output logits\n",
        "    3. Normalize the batch output\n",
        "    \"\"\"\n",
        "    \n",
        "    ### TO BE COMPLETED ###\n",
        "    tokens = self.tokenizer(sentences,padding=True,return_tensors=\"pt\")\n",
        "    model_output = self.model(**tokenized_sentences).last_hidden_state\n",
        "\n",
        "    return F.normalize(torch.mean(model_output, dim=1), dim=1).detach().numpy()\n",
        "\n",
        "\n",
        "bertIndex = FaissIndexer(sample_dataset,\n",
        "                  question_map,\n",
        "                  sample_eval_dataset,\n",
        "                  batch_size=32, \n",
        "                  sentence_vector_dim=768, \n",
        "                  vectorizer=BertVectorizer())\n",
        "\n",
        "bertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bLdQ4pQwo-m"
      },
      "source": [
        "### Model 3: Sentence Transformer --- TO BE COMPLETED\n",
        "##### <font color='red'>Expected recall@10: ~93%, MRR: ~0.34</font>\n",
        "\n",
        "Compute the sentence embeddings using the Sentence BERT model and complete the `vectorize` function. Feel free to look up documentation on https://www.sbert.net/. \n",
        "\n",
        "Implementation:\n",
        "\n",
        "1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
        "2. Normalize the batch output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpTt--KFTd3t"
      },
      "source": [
        "class SentenceBertVectorizer:\n",
        "  def __init__(self):\n",
        "    self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "  def vectorize(self, sentences):\n",
        "    \"\"\"Return sentence vectors for the batch of sentences. \n",
        "\n",
        "    1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
        "    2. Normalize the batch output\n",
        "    \"\"\"\n",
        "\n",
        "    ### TO BE COMPLETED ###\n",
        "    sentence_vectors = self.model.encode(sentences)\n",
        "\n",
        "    return sentence_vectors / np.expand_dims(np.linalg.norm(sentence_vectors, axis=1), axis=1)\n",
        "\n",
        "\n",
        "SBertIndex = FaissIndexer(sample_dataset,\n",
        "                  question_map,\n",
        "                  sample_eval_dataset,\n",
        "                  batch_size=1024, \n",
        "                  sentence_vector_dim=384, \n",
        "                  vectorizer=SentenceBertVectorizer())\n",
        "\n",
        "SBertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHhRKpknq5we"
      },
      "source": [
        "### Model 4: Cohere Sentence Embeddings --- TO BE COMPLETED\n",
        "##### <font color='red'>Expected recall@10: ~89%, MRR: ~0.34</font>\n",
        "\n",
        "Make sure create a Cohere account and make an API key.\n",
        "Compute the sentence embeddings using the cohere API and complete the `vectorize` function. Feel free to look up documentation on https://docs.cohere.ai/semantic-search. \n",
        "\n",
        "Implementation:\n",
        "\n",
        "1. Pipe the input sentences through the Cohere API. Make sure to select the small model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COHERE_API_KEY = \"YOUR COHERE API KEY\"\n",
        "co = cohere.Client(COHERE_API_KEY)"
      ],
      "metadata": {
        "id": "FODgVGURr2YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnYpcV8Cq5wk"
      },
      "source": [
        "class CohereVectorizer:\n",
        "  def vectorize(self, sentences):\n",
        "    \"\"\"Return sentence vectors for the batch of sentences. \n",
        "\n",
        "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
        "    2. Sentence vector is the mean of word vectors of each token\n",
        "    3. Stack the sentence vectors into a numpy array using np.stack\n",
        "    \"\"\"\n",
        "\n",
        "    ### TO BE COMPLETED ###\n",
        "    sentence_vectors = co.embed(texts=sentences,\n",
        "                                model='small',\n",
        "                                truncate='LEFT').embeddings\n",
        "    \n",
        "\n",
        "    # Convert from float64 to float32 to prevent bug:\n",
        "    # https://github.com/facebookresearch/faiss/issues/461\n",
        "    return np.float32(np.stack(sentence_vectors))\n",
        "\n",
        "\n",
        "cohereIndex = FaissIndexer(sample_dataset,\n",
        "                  question_map,\n",
        "                  sample_eval_dataset,\n",
        "                  batch_size=32, \n",
        "                  sentence_vector_dim=1024, \n",
        "                  vectorizer=CohereVectorizer())\n",
        "\n",
        "cohereIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onEbNcqnq5wl"
      },
      "source": [
        "ðŸŽ‰ CONGRATULATIONS on finishing the assignment!!! We built a real model with an actual datasets for a problem that is used every time a new Quora question gets created!! \n",
        "\n",
        "As for why did SentenceBERT & Cohere perform so well, we'll cover that in Siamese networks in week4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM0pHSbtGHuj"
      },
      "source": [
        "# Extensions\n",
        "\n",
        "Now that you've worked through the project there is a lot more for us to try:\n",
        "\n",
        "- See if you can use BERT to improve the model you shipped in Week 1.\n",
        "- Try out `SentenceBert` and `SpacyVectors` on the entire dataset rather the sample and see what you get?\n",
        "- Try different transformer models from hugging face"
      ]
    }
  ]
}